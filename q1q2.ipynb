{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "1k_6AaSdRroU"
      },
      "id": "1k_6AaSdRroU"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fdf35712",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf35712",
        "outputId": "ca20e458-c6af-4254-f5e1-660308b89283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             text                 code  workerid probid  \\\n",
            "0                             NaN         int main() {         1     3A   \n",
            "1                 create string s            string s;         1     3A   \n",
            "2  create integers x1, y1, x2, y2  int x1, y1, x2, y2;         1     3A   \n",
            "3                          read s            cin >> s;         1     3A   \n",
            "4             set x1 to s[0] - 96      x1 = s[0] - 96;         1     3A   \n",
            "\n",
            "      subid  line  indent  \n",
            "0  41470897     0       0  \n",
            "1  41470897     1       1  \n",
            "2  41470897     2       1  \n",
            "3  41470897     3       1  \n",
            "4  41470897     4       1  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' tokenizer data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# If 'punkt_tab' is still missing, try:\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Download punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "file_path = r\"/content/spoc-train.csv\" # Change to your dataset's path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display dataset information\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPROCESSING**"
      ],
      "metadata": {
        "id": "-u3-YY_2Rw_j"
      },
      "id": "-u3-YY_2Rw_j"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3a08429f",
      "metadata": {
        "id": "3a08429f"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80e52203",
      "metadata": {
        "id": "80e52203"
      },
      "outputs": [],
      "source": [
        "df[\"text\"] = df[\"text\"].fillna(\"\")  # Replace NaN with empty strings\n",
        "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3e73cd24",
      "metadata": {
        "id": "3e73cd24"
      },
      "outputs": [],
      "source": [
        "df[\"text_tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "123f02bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "123f02bd",
        "outputId": "de391cb8-bdfb-42b3-f89a-0ba1fce7508b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object\n",
            "0\n",
            "Empty DataFrame\n",
            "Columns: [text, code, workerid, probid, subid, line, indent, text_tokens]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "print(df[\"text\"].dtype)  # Check the column's data type\n",
        "print(df[\"text\"].isna().sum())  # Count missing values\n",
        "print(df[df[\"text\"].apply(lambda x: not isinstance(x, str))])  # Show non-string values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1527b229",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1527b229",
        "outputId": "79100c73-e568-472e-8a34-55eb72a8773e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer if not available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenizing pseudocode and code\n",
        "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)\n",
        "df[\"code_tokens\"] = df[\"code\"].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION**"
      ],
      "metadata": {
        "id": "ryLp8hNjR1XQ"
      },
      "id": "ryLp8hNjR1XQ"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3667d51e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3667d51e",
        "outputId": "68b75cc6-b378-45e4-c0c9-3deca2b445ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples from index 1-10:\n",
            "Index 1:\n",
            "Tokenized Pseudocode: ['create', 'string', 's']\n",
            "Tokenized C++ Code: ['string', 's', ';']\n",
            "--------------------------------------------------\n",
            "Index 2:\n",
            "Tokenized Pseudocode: ['create', 'integers', 'x1', ',', 'y1', ',', 'x2', ',', 'y2']\n",
            "Tokenized C++ Code: ['int', 'x1', ',', 'y1', ',', 'x2', ',', 'y2', ';']\n",
            "--------------------------------------------------\n",
            "Index 3:\n",
            "Tokenized Pseudocode: ['read', 's']\n",
            "Tokenized C++ Code: ['cin', '>', '>', 's', ';']\n",
            "--------------------------------------------------\n",
            "Index 4:\n",
            "Tokenized Pseudocode: ['set', 'x1', 'to', 's', '[', '0', ']', '-', '96']\n",
            "Tokenized C++ Code: ['x1', '=', 's', '[', '0', ']', '-', '96', ';']\n",
            "--------------------------------------------------\n",
            "Index 5:\n",
            "Tokenized Pseudocode: ['set', 'y1', 'to', 's', '[', '1', ']', '-', \"'\", '0', \"'\"]\n",
            "Tokenized C++ Code: ['y1', '=', 's', '[', '1', ']', '-', \"'\", '0', \"'\", ';']\n",
            "--------------------------------------------------\n",
            "Index 6:\n",
            "Tokenized Pseudocode: ['read', 's']\n",
            "Tokenized C++ Code: ['cin', '>', '>', 's', ';']\n",
            "--------------------------------------------------\n",
            "Index 7:\n",
            "Tokenized Pseudocode: ['set', 'x2', 'to', 's', '[', '0', ']', '-', '96']\n",
            "Tokenized C++ Code: ['x2', '=', 's', '[', '0', ']', '-', '96', ';']\n",
            "--------------------------------------------------\n",
            "Index 8:\n",
            "Tokenized Pseudocode: ['set', 'y2', 'to', 's', '[', '1', ']', '-', \"'\", '0', \"'\"]\n",
            "Tokenized C++ Code: ['y2', '=', 's', '[', '1', ']', '-', \"'\", '0', \"'\", ';']\n",
            "--------------------------------------------------\n",
            "Index 9:\n",
            "Tokenized Pseudocode: ['print', 'maximum', 'of', 'absolute', 'value', 'of', 'x1', '-', 'x2', 'and', 'absolute', 'value', 'of', 'y1', '-', 'y2', ',', 'print', 'newline']\n",
            "Tokenized C++ Code: ['cout', '<', '<', 'max', '(', 'abs', '(', 'x1', '-', 'x2', ')', ',', 'abs', '(', 'y1', '-', 'y2', ')', ')', '<', '<', 'endl', ';']\n",
            "--------------------------------------------------\n",
            "Index 10:\n",
            "Tokenized Pseudocode: ['while', 'x1', 'is', 'not', 'x2', 'or', 'y1', 'is', 'not', 'y2']\n",
            "Tokenized C++ Code: ['while', '(', 'x1', '!', '=', 'x2', '||', 'y1', '!', '=', 'y2', ')', '{']\n",
            "--------------------------------------------------\n",
            "\n",
            "Samples from index 20-30:\n",
            "Index 20:\n",
            "Tokenized Pseudocode: ['print', '``', 'D', \"''\"]\n",
            "Tokenized C++ Code: ['cout', '<', '<', '``', 'D', \"''\", ';']\n",
            "--------------------------------------------------\n",
            "Index 21:\n",
            "Tokenized Pseudocode: ['decrement', 'y1']\n",
            "Tokenized C++ Code: ['y1', '--', ';']\n",
            "--------------------------------------------------\n",
            "Index 22:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['}']\n",
            "--------------------------------------------------\n",
            "Index 23:\n",
            "Tokenized Pseudocode: ['if', 'y1', 'is', 'less', 'than', 'y2']\n",
            "Tokenized C++ Code: ['if', '(', 'y1', '<', 'y2', ')', '{']\n",
            "--------------------------------------------------\n",
            "Index 24:\n",
            "Tokenized Pseudocode: ['print', '``', 'U', \"''\"]\n",
            "Tokenized C++ Code: ['cout', '<', '<', '``', 'U', \"''\", ';']\n",
            "--------------------------------------------------\n",
            "Index 25:\n",
            "Tokenized Pseudocode: ['increment', 'y1']\n",
            "Tokenized C++ Code: ['y1++', ';']\n",
            "--------------------------------------------------\n",
            "Index 26:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['}']\n",
            "--------------------------------------------------\n",
            "Index 27:\n",
            "Tokenized Pseudocode: ['print', '``', '\\\\n', \"''\"]\n",
            "Tokenized C++ Code: ['cout', '<', '<', '``', '\\\\n', \"''\", ';']\n",
            "--------------------------------------------------\n",
            "Index 28:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['}']\n",
            "--------------------------------------------------\n",
            "Index 29:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['return', '0', ';']\n",
            "--------------------------------------------------\n",
            "Index 30:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['}']\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Print samples from index 1-10\n",
        "print(\"Samples from index 1-10:\")\n",
        "for i in range(1, 11):\n",
        "    print(f\"Index {i}:\")\n",
        "    print(\"Tokenized Pseudocode:\", df[\"text_tokens\"].iloc[i])\n",
        "    print(\"Tokenized C++ Code:\", df[\"code_tokens\"].iloc[i])\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Print samples from index 20-30\n",
        "print(\"\\nSamples from index 20-30:\")\n",
        "for i in range(20, 31):\n",
        "    print(f\"Index {i}:\")\n",
        "    print(\"Tokenized Pseudocode:\", df[\"text_tokens\"].iloc[i])\n",
        "    print(\"Tokenized C++ Code:\", df[\"code_tokens\"].iloc[i])\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Sd8I0TttbCaZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd8I0TttbCaZ",
        "outputId": "ddbefebc-3a90-4fbb-dd5b-031bf052c699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized data saved to tokenized_spoc.csv\n"
          ]
        }
      ],
      "source": [
        "# Save tokenized pseudocode and C++ code to CSV\n",
        "output_file = \"tokenized_spoc.csv\"\n",
        "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Tokenized data saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "-HAGLVzqXEQy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HAGLVzqXEQy",
        "outputId": "eacfc686-418a-4b19-a56d-8315f1e65916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated tokenized data saved to tokenized_spoc_with_tokens.csv\n"
          ]
        }
      ],
      "source": [
        "# Add start and end tokens to tokenized C++ code\n",
        "df[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: [\"<start>\"] + tokens + [\"<end>\"])\n",
        "\n",
        "# Save updated tokenized data to CSV\n",
        "output_file = \"tokenized_spoc_with_tokens.csv\"\n",
        "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Updated tokenized data saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6tRCwKS0X25B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tRCwKS0X25B",
        "outputId": "9c192946-958e-4241-96c5-058dbeacc5de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded tokenized data saved to tokenized_spoc_padded.csv\n"
          ]
        }
      ],
      "source": [
        "# Make \"text_tokens\" and \"code_tokens\" length same by padding with \"<pad>\"\n",
        "max_len = max(df[\"text_tokens\"].apply(len).max(), df[\"code_tokens\"].apply(len).max())\n",
        "\n",
        "df[\"text_tokens\"] = df[\"text_tokens\"].apply(lambda tokens: tokens + [\"<pad>\"] * (max_len - len(tokens)))\n",
        "df[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: tokens + [\"<pad>\"] * (max_len - len(tokens)))\n",
        "\n",
        "# Save padded tokenized data to CSV\n",
        "output_file = \"tokenized_spoc_padded.csv\"\n",
        "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Padded tokenized data saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "r4hbVXb5YI4-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4hbVXb5YI4-",
        "outputId": "16529d43-5b12-4fde-e2a7-d76a34d2b44e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary saved to vocabulary.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Define special tokens with fixed indices\n",
        "vocab = {\n",
        "    \"<unk>\": 0,\n",
        "    \"<pad>\": 1,\n",
        "    \"<start>\": 2,\n",
        "    \"<end>\": 3\n",
        "}\n",
        "\n",
        "# Assign indices to other tokens\n",
        "for column in [\"text_tokens\", \"code_tokens\"]:\n",
        "    for tokens in df[column]:\n",
        "        for token in tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = len(vocab)\n",
        "\n",
        "# Save vocabulary to JSON\n",
        "vocab_file = \"vocabulary.json\"\n",
        "with open(vocab_file, \"w\") as f:\n",
        "    json.dump(vocab, f, indent=4)\n",
        "\n",
        "print(f\"Vocabulary saved to {vocab_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "_TEFKw4KY6VO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TEFKw4KY6VO",
        "outputId": "58ec9b07-227a-4fe2-eacd-dafe6ab7b54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sequences saved to tokenized_sequences.csv\n"
          ]
        }
      ],
      "source": [
        "# Load vocabulary\n",
        "with open(\"vocabulary.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# Load tokenized data\n",
        "df = pd.read_csv(\"tokenized_spoc_padded.csv\")\n",
        "\n",
        "# Convert string tokens to lists\n",
        "df[\"text_tokens\"] = df[\"text_tokens\"].apply(eval)\n",
        "df[\"code_tokens\"] = df[\"code_tokens\"].apply(eval)\n",
        "\n",
        "# Convert tokens to sequences using vocabulary\n",
        "df[\"text_sequences\"] = df[\"text_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
        "df[\"code_sequences\"] = df[\"code_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
        "\n",
        "# Save sequences to CSV\n",
        "output_file = \"tokenized_sequences.csv\"\n",
        "df[[\"text_sequences\", \"code_sequences\"]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Tokenized sequences saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8_ySZqiqaHUD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_ySZqiqaHUD",
        "outputId": "1b2ab0bc-f466-49c2-d184-280bd345fd09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading Batches:   0%|          | 0/3846 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import ast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DataLoad(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        self.inputs = [ast.literal_eval(x) for x in df['text_sequences']]\n",
        "        self.outputs = [ast.literal_eval(x) for x in df['code_sequences']]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n",
        "        output_tensor = torch.tensor(self.outputs[idx], dtype=torch.int64)\n",
        "        return input_tensor, output_tensor\n",
        "\n",
        "def Add_Pad(batch):\n",
        "    inputs, outputs = zip(*batch)\n",
        "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n",
        "    return inputs, outputs\n",
        "\n",
        "# Load dataset and dataloader\n",
        "dataset = DataLoad(\"tokenized_sequences.csv\")\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=Add_Pad)\n",
        "\n",
        "# Iterate with progress bar\n",
        "data_iter = iter(dataloader)\n",
        "for batch in tqdm(dataloader, desc=\"Loading Batches\"):\n",
        "    features, labels = batch  # Get a batch of data\n",
        "    break  # Remove this if you want to iterate over all batches\n",
        "\n",
        "print(\"Batch loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "BeXmffD0bl4E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeXmffD0bl4E",
        "outputId": "65d5653a-50b3-49ac-9af8-fe55fc402e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73,   1,   1,  ...,   1,   1,   1],\n",
            "        [ 77, 686,  16,  ...,   1,   1,   1],\n",
            "        [ 44, 106,  92,  ...,   1,   1,   1],\n",
            "        ...,\n",
            "        [  4,   1,   1,  ...,   1,   1,   1],\n",
            "        [ 41,  78,   1,  ...,   1,   1,   1],\n",
            "        [  4,   1,   1,  ...,   1,   1,   1]])\n",
            "tensor([[   2,  328,   73,  ...,    1,    1,    1],\n",
            "        [   2,   77,   50,  ...,    1,    1,    1],\n",
            "        [   2,  106, 2367,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,  328,    3,  ...,    1,    1,    1],\n",
            "        [   2, 2823,  208,  ...,    1,    1,    1],\n",
            "        [   2,  328,    3,  ...,    1,    1,    1]])\n"
          ]
        }
      ],
      "source": [
        "print(features)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "azPgilarcWXf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azPgilarcWXf",
        "outputId": "a8f923d3-83b2-4dd5-eade-6a88f8c628ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Transformer Hyperparameters\n",
        "class Config:\n",
        "    vocab_size = 12006  # Adjust based on vocabulary.json\n",
        "    max_length = 100  # Adjust based on sequence length\n",
        "    embed_dim = 256\n",
        "    num_heads = 8\n",
        "    num_layers =2\n",
        "    feedforward_dim = 512\n",
        "    dropout = 0.1\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "# Transformer Model\n",
        "class PseudoCodeTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(PseudoCodeTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(config.embed_dim, config.max_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=config.embed_dim,\n",
        "            nhead=config.num_heads,\n",
        "            num_encoder_layers=config.num_layers,\n",
        "            num_decoder_layers=config.num_layers,\n",
        "            dim_feedforward=config.feedforward_dim,\n",
        "            dropout=config.dropout\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(config.embed_dim, config.vocab_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(config.device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.embedding(src) * math.sqrt(config.embed_dim)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(config.embed_dim)\n",
        "\n",
        "        src_emb = self.positional_encoding(src_emb)\n",
        "        tgt_emb = self.positional_encoding(tgt_emb)\n",
        "\n",
        "        src_mask = self.generate_square_subsequent_mask(src.size(1))\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
        "\n",
        "        out = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2),\n",
        "                               src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        out = self.fc_out(out.permute(1, 0, 2))  # Convert back to batch-first\n",
        "        return out\n",
        "\n",
        "# Initialize Model\n",
        "model = PseudoCodeTransformer(config).to(config.device)\n",
        "print(\"Model initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2XsYwb5jLxAT",
      "metadata": {
        "id": "2XsYwb5jLxAT"
      },
      "outputs": [],
      "source": [
        "def translate(model, pseudocode_tokens, vocab, device, max_length=50):\n",
        "        model.eval()\n",
        "\n",
        "        # Convert pseudocode tokens to numerical indices\n",
        "        input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in pseudocode_tokens]\n",
        "        input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "        # Start token for generation\n",
        "        output_ids = [vocab[\"<start>\"]]\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            output_tensor = torch.tensor(output_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            with torch.no_grad():\n",
        "                predictions = model(input_tensor, output_tensor)\n",
        "\n",
        "            # Select the most probable token\n",
        "            next_token_id = predictions.argmax(dim=-1)[:, -1].item()\n",
        "            output_ids.append(next_token_id)\n",
        "\n",
        "            # Stop if end token is generated\n",
        "            if next_token_id == vocab[\"<end>\"]:\n",
        "                break\n",
        "\n",
        "        # Convert token indices back to words\n",
        "        id_to_token = {idx: token for token, idx in vocab.items()}\n",
        "        translated_code = [id_to_token.get(idx, \"<unk>\") for idx in output_ids[1:]]  # Exclude <start> token\n",
        "\n",
        "        return \" \".join(translated_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "OJBeh_zNL6ZM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJBeh_zNL6ZM",
        "outputId": "5272bdd2-bda7-4413-e520-e1a01045a959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Vocabulary loaded with 12006 tokens\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load vocabulary\n",
        "with open(\"vocabulary.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# Ensure vocab is a dictionary\n",
        "print(f\"âœ… Vocabulary loaded with {len(vocab)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PSEUDOCODE TO C++**"
      ],
      "metadata": {
        "id": "4_qxbTo6RZjv"
      },
      "id": "4_qxbTo6RZjv"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "SUOIS04idMXB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUOIS04idMXB",
        "outputId": "97ba3f27-4459-475b-ef8f-440eabc5d541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3846/3846 [06:17<00:00, 10.20it/s, loss=0.248]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.9357\n",
            "âœ… Model saved: checkpoints/p2c1.pth\n",
            "ðŸ”¹ Example Prediction (Pseudocode â†’ C++): int x , int x , int x , int x , int x , int x , int x int x , int x , int x , int x , int x , int x , int x , int x int x , int x , int\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¹ Using device: {device}\")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Loss Function & Optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=1)  # Ignore padding token\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
        "\n",
        "# Create directory to save models\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "        src, tgt = batch\n",
        "        src, tgt = src.to(device), tgt.to(device)  # Move batch to GPU\n",
        "\n",
        "        tgt_input = tgt[:, :-1]  # Remove <end> token\n",
        "        tgt_output = tgt[:, 1:]  # Shifted version\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "\n",
        "        loss = criterion(output.view(-1, config.vocab_size), tgt_output.contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save Model Checkpoint\n",
        "    torch.save(model.state_dict(), f\"checkpoints/p2c{epoch+1}.pth\")\n",
        "    print(f\"âœ… Model saved: checkpoints/p2c{epoch+1}.pth\")\n",
        "\n",
        "    # Print Example Prediction\n",
        "    model.eval()\n",
        "    example_pseudocode = [\"create\", \"integer\", \"x\"]\n",
        "    translated_code = translate(model, example_pseudocode, vocab, device)\n",
        "    print(f\"ðŸ”¹ Example Prediction (Pseudocode â†’ C++): {translated_code}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "D5pxkdcUL5Gd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5pxkdcUL5Gd",
        "outputId": "d131f1f9-afac-4ed3-a88e-56b390b99e4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PseudoCodeTransformer(\n",
              "  (embedding): Embedding(12006, 256)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (fc_out): Linear(in_features=256, out_features=12006, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "t-xzQokaPy_E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-xzQokaPy_E",
        "outputId": "415c0b5f-f384-4f1e-a4c7-e715563af0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Using device: cuda\n",
            "ðŸ”¹ Example Prediction (Pseudocode â†’ C++): for ( int i = 0 ; i < ans.size ( ( ) ; i++ ) { cout < < ans ans ans ( ans ( ans ( ans ( ans ) ) ; } <end>\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-d110df5ce25d>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/checkpoints/p2c1.pth\", map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¹ Using device: {device}\")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "model = PseudoCodeTransformer(config).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/checkpoints/p2c1.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Run translation on example pseudocode\n",
        "example_pseudocode = [\"for\", \"i\", \"=\", \"0\", \"to\", \"size\", \"of\", \"ans\", \"exclusive\", \",\", \"print\", \"ans\", \"[\", \"i\", \"]\", \"print\", \"newline\"]\n",
        "translated_code = translate(model, example_pseudocode, vocab, device)\n",
        "\n",
        "print(f\"ðŸ”¹ Example Prediction (Pseudocode â†’ C++): {translated_code}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CrE_8fkdRdfQ",
      "metadata": {
        "id": "CrE_8fkdRdfQ"
      },
      "source": [
        "**C++ CODE TO PSEUDOCODE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "yE-UUuDGI5Az",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE-UUuDGI5Az",
        "outputId": "a0bebb33-4d16-4d6e-b203-8854c4e7dc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ C++ â†’ Pseudocode Model initialized!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import ast\n",
        "import json\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Transformer Hyperparameters\n",
        "class Config:\n",
        "    vocab_size = 12006  # Adjust based on vocabulary.json\n",
        "    max_length = 100\n",
        "    embed_dim = 256\n",
        "    num_heads = 8\n",
        "    num_layers = 2\n",
        "    feedforward_dim = 512\n",
        "    dropout = 0.1\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "# Transformer Model\n",
        "class CPPtoPseudoTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CPPtoPseudoTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(config.embed_dim, config.max_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=config.embed_dim,\n",
        "            nhead=config.num_heads,\n",
        "            num_encoder_layers=config.num_layers,\n",
        "            num_decoder_layers=config.num_layers,\n",
        "            dim_feedforward=config.feedforward_dim,\n",
        "            dropout=config.dropout\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(config.embed_dim, config.vocab_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(config.device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.embedding(src) * math.sqrt(config.embed_dim)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(config.embed_dim)\n",
        "\n",
        "        src_emb = self.positional_encoding(src_emb)\n",
        "        tgt_emb = self.positional_encoding(tgt_emb)\n",
        "\n",
        "        src_mask = self.generate_square_subsequent_mask(src.size(1))\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
        "\n",
        "        out = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2),\n",
        "                               src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        out = self.fc_out(out.permute(1, 0, 2))\n",
        "        return out\n",
        "\n",
        "# Initialize Model\n",
        "model = CPPtoPseudoTransformer(config).to(config.device)\n",
        "print(\"ðŸš€ C++ â†’ Pseudocode Model initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "CdWsUGr4KHM1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdWsUGr4KHM1",
        "outputId": "28fe6d7e-fe9c-4e04-fd40-c25fc349bff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Vocabulary loaded with 12006 tokens\n"
          ]
        }
      ],
      "source": [
        "# Load Vocabulary\n",
        "with open(\"vocabulary.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "print(f\"âœ… Vocabulary loaded with {len(vocab)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "BEEQ_zNHKQRY",
      "metadata": {
        "id": "BEEQ_zNHKQRY"
      },
      "outputs": [],
      "source": [
        "def translate2(model, cpp_tokens, vocab, device, max_length=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert C++ tokens to numerical indices\n",
        "    input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in cpp_tokens]\n",
        "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    output_ids = [vocab[\"<start>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        output_tensor = torch.tensor(output_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_tensor, output_tensor)\n",
        "\n",
        "        # Select the most probable token\n",
        "        next_token_id = predictions.argmax(dim=-1)[:, -1].item()\n",
        "\n",
        "        if next_token_id == vocab[\"<pad>\"]:  # Ignore <pad> tokens\n",
        "            continue\n",
        "\n",
        "        output_ids.append(next_token_id)\n",
        "\n",
        "        if next_token_id == vocab[\"<end>\"]:  # Stop if <end> is generated\n",
        "            break\n",
        "\n",
        "    # Convert token indices back to words\n",
        "    id_to_token = {idx: token for token, idx in vocab.items()}\n",
        "    translated_pseudocode = [id_to_token.get(idx, \"<unk>\") for idx in output_ids[1:]]  # Exclude <start>\n",
        "\n",
        "    return \" \".join(translated_pseudocode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "t3HIGFZCKNaB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3HIGFZCKNaB",
        "outputId": "b719bf9f-6cc4-49b2-da31-8332f018a5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded 246086 examples for training\n"
          ]
        }
      ],
      "source": [
        "# Dataset Class\n",
        "class CPPToPseudoDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        self.inputs = [ast.literal_eval(x) for x in df['code_sequences']]\n",
        "        self.outputs = [ast.literal_eval(x) for x in df['text_sequences']]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n",
        "        output_tensor = torch.tensor([vocab[\"<start>\"]] + self.outputs[idx] + [vocab[\"<end>\"]], dtype=torch.int64)\n",
        "        return input_tensor, output_tensor\n",
        "\n",
        "# Padding Function\n",
        "def Add_Pad(batch):\n",
        "    inputs, outputs = zip(*batch)\n",
        "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n",
        "    return inputs, outputs\n",
        "\n",
        "# Load Dataset\n",
        "dataset = CPPToPseudoDataset(\"tokenized_sequences.csv\")\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=Add_Pad)\n",
        "\n",
        "print(f\"âœ… Loaded {len(dataset)} examples for training\")\n",
        "\n",
        "# Training Configuration\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
        "\n",
        "# Create directory to save models\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "QXd9vz5EKT8u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXd9vz5EKT8u",
        "outputId": "15da7fa5-cb10-4fc3-8955-3a52396733c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3846/3846 [06:27<00:00,  9.93it/s, loss=0.358]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 1.4150\n",
            "âœ… Model saved: checkpoints/c2p1.pth\n",
            "ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): integer function isprime with integer argument integer argument size of size of size of size of size of size of a integer as integer as integer casted size of size of size of size of size of a integer as integer casted size of size of size of size of\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "        src, tgt = batch\n",
        "        src, tgt = src.to(config.device), tgt.to(config.device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "\n",
        "        loss = criterion(output.view(-1, config.vocab_size), tgt_output.contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save Model Checkpoint\n",
        "    torch.save(model.state_dict(), f\"checkpoints/c2p{epoch+1}.pth\")\n",
        "    print(f\"âœ… Model saved: checkpoints/c2p{epoch+1}.pth\")\n",
        "\n",
        "    # Print Example Prediction\n",
        "    model.eval()\n",
        "    example_cpp = [\"int\", \"main\", \"(\", \")\", \"{\", \"return\", \"0\", \";\", \"}\"]\n",
        "    translated_pseudocode = translate2(model, example_cpp, vocab, config.device)\n",
        "    print(f\"ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): {translated_pseudocode}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "tfjIcOTkK33z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfjIcOTkK33z",
        "outputId": "4d923865-b799-4144-ab9f-b0f0266d4751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "<ipython-input-37-e3fbe37d9be4>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/checkpoints/c2p1.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): a = integer , a = 10 = 10 = 10 = 10 integer with a = 10 = 10 = 10 = 10 = 10 = 10 = 10 = 10 integer with a = 10 = 10 = 10 = 10 = 10 = 10 = 10 integer\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¹ Using device: {device}\")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "model = PseudoCodeTransformer(config).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/checkpoints/c2p1.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "example_cpp = [\"int\", \"a\",\"=\", \"10\",\";\"]\n",
        "translated_pseudocode = translate2(model, example_cpp, vocab, config.device)\n",
        "print(f\"ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): {translated_pseudocode}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}