{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae62f55",
   "metadata": {
    "id": "4ae62f55"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf35712",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdf35712",
    "outputId": "aba7cfe3-c992-452c-db12-378eea703c32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaiiikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shaiiikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaiiikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             text                 code  workerid probid  \\\n",
      "0                             NaN         int main() {         1     3A   \n",
      "1                 create string s            string s;         1     3A   \n",
      "2  create integers x1, y1, x2, y2  int x1, y1, x2, y2;         1     3A   \n",
      "3                          read s            cin >> s;         1     3A   \n",
      "4             set x1 to s[0] - 96      x1 = s[0] - 96;         1     3A   \n",
      "\n",
      "      subid  line  indent  \n",
      "0  41470897     0       0  \n",
      "1  41470897     1       1  \n",
      "2  41470897     2       1  \n",
      "3  41470897     3       1  \n",
      "4  41470897     4       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the 'punkt' tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# If 'punkt_tab' is still missing, try:\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "# Download punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:/Users/shaiiikh/py/A3/spoc-train.csv\" # Change to your dataset's path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display dataset information\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6889f401",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6889f401",
    "outputId": "55269b88-418b-4fd5-d4ed-065109f899e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 246086 entries, 0 to 246085\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   text      181862 non-null  object\n",
      " 1   code      246086 non-null  object\n",
      " 2   workerid  246086 non-null  int64 \n",
      " 3   probid    246086 non-null  object\n",
      " 4   subid     246086 non-null  int64 \n",
      " 5   line      246086 non-null  int64 \n",
      " 6   indent    246086 non-null  int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 13.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ad88e",
   "metadata": {
    "id": "dd3ad88e"
   },
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a08429f",
   "metadata": {
    "id": "3a08429f"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e52203",
   "metadata": {
    "id": "80e52203"
   },
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].fillna(\"\")  # Replace NaN with empty strings\n",
    "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e73cd24",
   "metadata": {
    "id": "3e73cd24"
   },
   "outputs": [],
   "source": [
    "df[\"text_tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "123f02bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "123f02bd",
    "outputId": "cf09d02c-e55b-4857-b46f-1aee035bab36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [text, code, workerid, probid, subid, line, indent, text_tokens]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"].dtype)  # Check the column's data type\n",
    "print(df[\"text\"].isna().sum())  # Count missing values\n",
    "print(df[df[\"text\"].apply(lambda x: not isinstance(x, str))])  # Show non-string values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666823c",
   "metadata": {
    "id": "7666823c"
   },
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1527b229",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1527b229",
    "outputId": "3911e731-9c6a-40a7-f625-e59d4491d13e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaiiikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download tokenizer if not available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizing pseudocode and code\n",
    "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)\n",
    "df[\"code_tokens\"] = df[\"code\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8e137",
   "metadata": {
    "id": "62e8e137"
   },
   "source": [
    "# PRINT TOKEN SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3667d51e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3667d51e",
    "outputId": "9619ff63-be61-467c-a1a4-0cf13b14fced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples from index 1-10:\n",
      "Index 1:\n",
      "Tokenized Pseudocode: ['create', 'string', 's']\n",
      "Tokenized C++ Code: ['string', 's', ';']\n",
      "--------------------------------------------------\n",
      "Index 2:\n",
      "Tokenized Pseudocode: ['create', 'integers', 'x1', ',', 'y1', ',', 'x2', ',', 'y2']\n",
      "Tokenized C++ Code: ['int', 'x1', ',', 'y1', ',', 'x2', ',', 'y2', ';']\n",
      "--------------------------------------------------\n",
      "Index 3:\n",
      "Tokenized Pseudocode: ['read', 's']\n",
      "Tokenized C++ Code: ['cin', '>', '>', 's', ';']\n",
      "--------------------------------------------------\n",
      "Index 4:\n",
      "Tokenized Pseudocode: ['set', 'x1', 'to', 's', '[', '0', ']', '-', '96']\n",
      "Tokenized C++ Code: ['x1', '=', 's', '[', '0', ']', '-', '96', ';']\n",
      "--------------------------------------------------\n",
      "Index 5:\n",
      "Tokenized Pseudocode: ['set', 'y1', 'to', 's', '[', '1', ']', '-', \"'\", '0', \"'\"]\n",
      "Tokenized C++ Code: ['y1', '=', 's', '[', '1', ']', '-', \"'\", '0', \"'\", ';']\n",
      "--------------------------------------------------\n",
      "Index 6:\n",
      "Tokenized Pseudocode: ['read', 's']\n",
      "Tokenized C++ Code: ['cin', '>', '>', 's', ';']\n",
      "--------------------------------------------------\n",
      "Index 7:\n",
      "Tokenized Pseudocode: ['set', 'x2', 'to', 's', '[', '0', ']', '-', '96']\n",
      "Tokenized C++ Code: ['x2', '=', 's', '[', '0', ']', '-', '96', ';']\n",
      "--------------------------------------------------\n",
      "Index 8:\n",
      "Tokenized Pseudocode: ['set', 'y2', 'to', 's', '[', '1', ']', '-', \"'\", '0', \"'\"]\n",
      "Tokenized C++ Code: ['y2', '=', 's', '[', '1', ']', '-', \"'\", '0', \"'\", ';']\n",
      "--------------------------------------------------\n",
      "Index 9:\n",
      "Tokenized Pseudocode: ['print', 'maximum', 'of', 'absolute', 'value', 'of', 'x1', '-', 'x2', 'and', 'absolute', 'value', 'of', 'y1', '-', 'y2', ',', 'print', 'newline']\n",
      "Tokenized C++ Code: ['cout', '<', '<', 'max', '(', 'abs', '(', 'x1', '-', 'x2', ')', ',', 'abs', '(', 'y1', '-', 'y2', ')', ')', '<', '<', 'endl', ';']\n",
      "--------------------------------------------------\n",
      "Index 10:\n",
      "Tokenized Pseudocode: ['while', 'x1', 'is', 'not', 'x2', 'or', 'y1', 'is', 'not', 'y2']\n",
      "Tokenized C++ Code: ['while', '(', 'x1', '!', '=', 'x2', '||', 'y1', '!', '=', 'y2', ')', '{']\n",
      "--------------------------------------------------\n",
      "\n",
      "Samples from index 20-30:\n",
      "Index 20:\n",
      "Tokenized Pseudocode: ['print', '``', 'D', \"''\"]\n",
      "Tokenized C++ Code: ['cout', '<', '<', '``', 'D', \"''\", ';']\n",
      "--------------------------------------------------\n",
      "Index 21:\n",
      "Tokenized Pseudocode: ['decrement', 'y1']\n",
      "Tokenized C++ Code: ['y1', '--', ';']\n",
      "--------------------------------------------------\n",
      "Index 22:\n",
      "Tokenized Pseudocode: ['nan']\n",
      "Tokenized C++ Code: ['}']\n",
      "--------------------------------------------------\n",
      "Index 23:\n",
      "Tokenized Pseudocode: ['if', 'y1', 'is', 'less', 'than', 'y2']\n",
      "Tokenized C++ Code: ['if', '(', 'y1', '<', 'y2', ')', '{']\n",
      "--------------------------------------------------\n",
      "Index 24:\n",
      "Tokenized Pseudocode: ['print', '``', 'U', \"''\"]\n",
      "Tokenized C++ Code: ['cout', '<', '<', '``', 'U', \"''\", ';']\n",
      "--------------------------------------------------\n",
      "Index 25:\n",
      "Tokenized Pseudocode: ['increment', 'y1']\n",
      "Tokenized C++ Code: ['y1++', ';']\n",
      "--------------------------------------------------\n",
      "Index 26:\n",
      "Tokenized Pseudocode: ['nan']\n",
      "Tokenized C++ Code: ['}']\n",
      "--------------------------------------------------\n",
      "Index 27:\n",
      "Tokenized Pseudocode: ['print', '``', '\\\\n', \"''\"]\n",
      "Tokenized C++ Code: ['cout', '<', '<', '``', '\\\\n', \"''\", ';']\n",
      "--------------------------------------------------\n",
      "Index 28:\n",
      "Tokenized Pseudocode: ['nan']\n",
      "Tokenized C++ Code: ['}']\n",
      "--------------------------------------------------\n",
      "Index 29:\n",
      "Tokenized Pseudocode: ['nan']\n",
      "Tokenized C++ Code: ['return', '0', ';']\n",
      "--------------------------------------------------\n",
      "Index 30:\n",
      "Tokenized Pseudocode: ['nan']\n",
      "Tokenized C++ Code: ['}']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print samples from index 1-10\n",
    "print(\"Samples from index 1-10:\")\n",
    "for i in range(1, 11):\n",
    "    print(f\"Index {i}:\")\n",
    "    print(\"Tokenized Pseudocode:\", df[\"text_tokens\"].iloc[i])\n",
    "    print(\"Tokenized C++ Code:\", df[\"code_tokens\"].iloc[i])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print samples from index 20-30\n",
    "print(\"\\nSamples from index 20-30:\")\n",
    "for i in range(20, 31):\n",
    "    print(f\"Index {i}:\")\n",
    "    print(\"Tokenized Pseudocode:\", df[\"text_tokens\"].iloc[i])\n",
    "    print(\"Tokenized C++ Code:\", df[\"code_tokens\"].iloc[i])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "Sd8I0TttbCaZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sd8I0TttbCaZ",
    "outputId": "05541ebd-454f-455d-e2a3-57f6df689997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to tokenized_spoc.csv\n"
     ]
    }
   ],
   "source": [
    "# Save tokenized pseudocode and C++ code to CSV\n",
    "output_file = \"tokenized_spoc.csv\"\n",
    "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Tokenized data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "-HAGLVzqXEQy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HAGLVzqXEQy",
    "outputId": "59f572b7-bf31-4171-9112-2edec63c3937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated tokenized data saved to tokenized_spoc_with_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "# Add start and end tokens to tokenized C++ code\n",
    "df[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: [\"<start>\"] + tokens + [\"<end>\"])\n",
    "\n",
    "# Save updated tokenized data to CSV\n",
    "output_file = \"tokenized_spoc_with_tokens.csv\"\n",
    "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated tokenized data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6tRCwKS0X25B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tRCwKS0X25B",
    "outputId": "32366e38-5c7b-4c45-cc7a-6413dc6df8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded tokenized data saved to tokenized_spoc_padded.csv\n"
     ]
    }
   ],
   "source": [
    "# Make \"text_tokens\" and \"code_tokens\" length same by padding with \"<pad>\"\n",
    "max_len = max(df[\"text_tokens\"].apply(len).max(), df[\"code_tokens\"].apply(len).max())\n",
    "\n",
    "df[\"text_tokens\"] = df[\"text_tokens\"].apply(lambda tokens: tokens + [\"<pad>\"] * (max_len - len(tokens)))\n",
    "df[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: tokens + [\"<pad>\"] * (max_len - len(tokens)))\n",
    "\n",
    "# Save padded tokenized data to CSV\n",
    "output_file = \"tokenized_spoc_padded.csv\"\n",
    "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Padded tokenized data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "r4hbVXb5YI4-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4hbVXb5YI4-",
    "outputId": "36805383-5738-4634-fc97-472fd68c399c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to vocabulary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define special tokens with fixed indices\n",
    "vocab = {\n",
    "    \"<unk>\": 0,\n",
    "    \"<pad>\": 1,\n",
    "    \"<start>\": 2,\n",
    "    \"<end>\": 3\n",
    "}\n",
    "\n",
    "# Assign indices to other tokens\n",
    "for column in [\"text_tokens\", \"code_tokens\"]:\n",
    "    for tokens in df[column]:\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "# Save vocabulary to JSON\n",
    "vocab_file = \"vocabulary.json\"\n",
    "with open(vocab_file, \"w\") as f:\n",
    "    json.dump(vocab, f, indent=4)\n",
    "\n",
    "print(f\"Vocabulary saved to {vocab_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "_TEFKw4KY6VO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TEFKw4KY6VO",
    "outputId": "53b130fa-6c16-4356-9979-3a87b00e53ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequences saved to tokenized_sequences.csv\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary\n",
    "with open(\"vocabulary.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Load tokenized data\n",
    "df = pd.read_csv(\"C:/Users/shaiiikh/py/A3/tokenized_spoc_padded.csv\")\n",
    "\n",
    "# Convert string tokens to lists\n",
    "df[\"text_tokens\"] = df[\"text_tokens\"].apply(eval)\n",
    "df[\"code_tokens\"] = df[\"code_tokens\"].apply(eval)\n",
    "\n",
    "# Convert tokens to sequences using vocabulary\n",
    "df[\"text_sequences\"] = df[\"text_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
    "df[\"code_sequences\"] = df[\"code_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
    "\n",
    "# Save sequences to CSV\n",
    "output_file = \"tokenized_sequences.csv\"\n",
    "df[[\"text_sequences\", \"code_sequences\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Tokenized sequences saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8_ySZqiqaHUD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_ySZqiqaHUD",
    "outputId": "87124b59-8b7b-42ba-bba0-9c9a0ca1926c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Batches:   0%|          | 0/3846 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataLoad(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.inputs = [ast.literal_eval(x) for x in df['text_sequences']]\n",
    "        self.outputs = [ast.literal_eval(x) for x in df['code_sequences']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n",
    "        output_tensor = torch.tensor(self.outputs[idx], dtype=torch.int64)\n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "def Add_Pad(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n",
    "    return inputs, outputs\n",
    "\n",
    "# Load dataset and dataloader\n",
    "dataset = DataLoad(\"C:/Users/shaiiikh/py/A3/tokenized_sequences.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=Add_Pad)\n",
    "\n",
    "# Iterate with progress bar\n",
    "data_iter = iter(dataloader)\n",
    "for batch in tqdm(dataloader, desc=\"Loading Batches\"):\n",
    "    features, labels = batch  # Get a batch of data\n",
    "    break  # Remove this if you want to iterate over all batches\n",
    "\n",
    "print(\"Batch loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "BeXmffD0bl4E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeXmffD0bl4E",
    "outputId": "072ef76d-d277-4357-c108-b7529cc5cc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  35,  354,   32,  ...,    1,    1,    1],\n",
      "        [  35,  104,  719,  ...,    1,    1,    1],\n",
      "        [8002,   48,   22,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   4,    1,    1,  ...,    1,    1,    1],\n",
      "        [  15, 1438,   16,  ...,    1,    1,    1],\n",
      "        [   4,    1,    1,  ...,    1,    1,    1]])\n",
      "tensor([[   2,   35,   50,  ...,    1,    1,    1],\n",
      "        [   2,   35,   50,  ...,    1,    1,    1],\n",
      "        [   2,  103,  103,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,  328,    3,  ...,    1,    1,    1],\n",
      "        [   2, 1438,   48,  ...,    1,    1,    1],\n",
      "        [   2,  173,   18,  ...,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "azPgilarcWXf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azPgilarcWXf",
    "outputId": "143ee579-1fbf-4ff8-b54f-e4b2116245e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaiiikh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Transformer Hyperparameters\n",
    "class Config:\n",
    "    vocab_size = 12006  # Adjust based on vocabulary.json\n",
    "    max_length = 100  # Adjust based on sequence length\n",
    "    embed_dim = 256\n",
    "    num_heads = 8\n",
    "    num_layers =2\n",
    "    feedforward_dim = 512\n",
    "    dropout = 0.1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Transformer Model\n",
    "class PseudoCodeTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PseudoCodeTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(config.embed_dim, config.max_length)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.embed_dim,\n",
    "            nhead=config.num_heads,\n",
    "            num_encoder_layers=config.num_layers,\n",
    "            num_decoder_layers=config.num_layers,\n",
    "            dim_feedforward=config.feedforward_dim,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(config.embed_dim, config.vocab_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(config.device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src) * math.sqrt(config.embed_dim)\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(config.embed_dim)\n",
    "\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1))\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
    "\n",
    "        out = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2),\n",
    "                               src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        out = self.fc_out(out.permute(1, 0, 2))  # Convert back to batch-first\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = PseudoCodeTransformer(config).to(config.device)\n",
    "print(\"Model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2XsYwb5jLxAT",
   "metadata": {
    "id": "2XsYwb5jLxAT"
   },
   "outputs": [],
   "source": [
    "    def translate(model, pseudocode_tokens, vocab, device, max_length=50):\n",
    "        model.eval()\n",
    "    \n",
    "        # Convert pseudocode tokens to numerical indices\n",
    "        input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in pseudocode_tokens]\n",
    "        input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "        # Start token for generation\n",
    "        output_ids = [vocab[\"<start>\"]]\n",
    "    \n",
    "        for _ in range(max_length):\n",
    "            output_tensor = torch.tensor(output_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "            # Get model predictions\n",
    "            with torch.no_grad():\n",
    "                predictions = model(input_tensor, output_tensor)\n",
    "    \n",
    "            # Select the most probable token\n",
    "            next_token_id = predictions.argmax(dim=-1)[:, -1].item()\n",
    "            output_ids.append(next_token_id)\n",
    "    \n",
    "            # Stop if end token is generated\n",
    "            if next_token_id == vocab[\"<end>\"]:\n",
    "                break\n",
    "    \n",
    "        # Convert token indices back to words\n",
    "        id_to_token = {idx: token for token, idx in vocab.items()}\n",
    "        translated_code = [id_to_token.get(idx, \"<unk>\") for idx in output_ids[1:]]  # Exclude <start> token\n",
    "    \n",
    "        return \" \".join(translated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "OJBeh_zNL6ZM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJBeh_zNL6ZM",
    "outputId": "44bee4c9-e9ea-4c09-fa91-a25e4efb3e7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocabulary loaded with 12006 tokens\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load vocabulary\n",
    "with open(\"vocabulary.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Ensure vocab is a dictionary\n",
    "print(f\"✅ Vocabulary loaded with {len(vocab)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "SUOIS04idMXB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUOIS04idMXB",
    "outputId": "9134b609-78d4-4f07-bd4a-7057ba9b5adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3846/3846 [05:27<00:00, 11.73it/s, loss=0.942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.9262\n",
      "✅ Model saved: checkpoints/transformer_epoch_1.pth\n",
      "🔹 Example Prediction (Pseudocode → C++): int x x = int x x x ; <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔹 Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Loss Function & Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)  # Ignore padding token\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "\n",
    "# Create directory to save models\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)  # Move batch to GPU\n",
    "\n",
    "        tgt_input = tgt[:, :-1]  # Remove <end> token\n",
    "        tgt_output = tgt[:, 1:]  # Shifted version\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "\n",
    "        loss = criterion(output.view(-1, config.vocab_size), tgt_output.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save Model Checkpoint\n",
    "    torch.save(model.state_dict(), f\"checkpoints/transformer_epoch_{epoch+1}.pth\")\n",
    "    print(f\"✅ Model saved: checkpoints/transformer_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    # Print Example Prediction\n",
    "    model.eval()\n",
    "    example_pseudocode = [\"create\", \"integer\", \"x\"]\n",
    "    translated_code = translate(model, example_pseudocode, vocab, device)\n",
    "    print(f\"🔹 Example Prediction (Pseudocode → C++): {translated_code}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "D5pxkdcUL5Gd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5pxkdcUL5Gd",
    "outputId": "c479d4a4-cfde-48cb-d0e7-22c74db2a500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Example Prediction (Pseudocode → C++): for ( int i = 0 ; i < ans.size ( ) ; i++ ) { cout < < ans < ans [ i ] < ans [ ans [ ans [ i ] < ans [ ans ] < ans [ ans ] < ans ] < < ans\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_pseudocode = [\"for\", \"i\", \"=\", \"0\", \"to\", \"size\", \"of\", \"ans\", \"exclusive\", \",\", \"print\", \"ans\", \"[\", \"i\", \"]\", \"print\", \"newline\"]\n",
    "translated_code = translate(model, example_pseudocode, vocab, device)\n",
    "print(f\"🔹 Example Prediction (Pseudocode → C++): {translated_code}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "t-xzQokaPy_E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-xzQokaPy_E",
    "outputId": "9bdff1cd-1094-4ba8-df9f-9cbde4a628bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaiiikh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Example Prediction (Pseudocode → C++): for i = 0 to num num num [ i ] = num num [ i ] , ans = num [ i ] [ i ] ] ] [ ans ] ] ] [ ans ] ] [ ans ] ] ] [ ans ] ] [ ans ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔹 Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "model = PseudoCodeTransformer(config).to(device)\n",
    "model.load_state_dict(torch.load(\"C:/Users/shaiiikh/py/A3/cpp_to_pseudo_epoch_1.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Run translation on example pseudocode\n",
    "example_pseudocode = [\"for\", \"i\", \"=\", \"0\", \"to\", \"size\", \"of\", \"ans\", \"exclusive\", \",\", \"print\", \"ans\", \"[\", \"i\", \"]\", \"print\", \"newline\"]\n",
    "translated_code = translate(model, example_pseudocode, vocab, device)\n",
    "\n",
    "print(f\"🔹 Example Prediction (Pseudocode → C++): {translated_code}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CrE_8fkdRdfQ",
   "metadata": {
    "id": "CrE_8fkdRdfQ"
   },
   "source": [
    "**C++ CODE TO PSEUDOCODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "yE-UUuDGI5Az",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yE-UUuDGI5Az",
    "outputId": "8513b586-2c83-4a62-f86a-5bb00f3247a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 C++ → Pseudocode Model initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaiiikh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Transformer Hyperparameters\n",
    "class Config:\n",
    "    vocab_size = 12006  # Adjust based on vocabulary.json\n",
    "    max_length = 100\n",
    "    embed_dim = 256\n",
    "    num_heads = 8\n",
    "    num_layers = 2\n",
    "    feedforward_dim = 512\n",
    "    dropout = 0.1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Transformer Model\n",
    "class CPPtoPseudoTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CPPtoPseudoTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(config.embed_dim, config.max_length)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.embed_dim,\n",
    "            nhead=config.num_heads,\n",
    "            num_encoder_layers=config.num_layers,\n",
    "            num_decoder_layers=config.num_layers,\n",
    "            dim_feedforward=config.feedforward_dim,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(config.embed_dim, config.vocab_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(config.device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src) * math.sqrt(config.embed_dim)\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(config.embed_dim)\n",
    "\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1))\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
    "\n",
    "        out = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2),\n",
    "                               src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        out = self.fc_out(out.permute(1, 0, 2))\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = CPPtoPseudoTransformer(config).to(config.device)\n",
    "print(\"🚀 C++ → Pseudocode Model initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "CdWsUGr4KHM1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdWsUGr4KHM1",
    "outputId": "b88217be-45e8-4c85-eed9-679b54627848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocabulary loaded with 12006 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load Vocabulary\n",
    "with open(\"vocabulary.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print(f\"✅ Vocabulary loaded with {len(vocab)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "BEEQ_zNHKQRY",
   "metadata": {
    "id": "BEEQ_zNHKQRY"
   },
   "outputs": [],
   "source": [
    "def translate2(model, cpp_tokens, vocab, device, max_length=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert C++ tokens to numerical indices\n",
    "    input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in cpp_tokens]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    output_ids = [vocab[\"<start>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output_tensor = torch.tensor(output_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor, output_tensor)\n",
    "\n",
    "        # Select the most probable token\n",
    "        next_token_id = predictions.argmax(dim=-1)[:, -1].item()\n",
    "\n",
    "        if next_token_id == vocab[\"<pad>\"]:  # Ignore <pad> tokens\n",
    "            continue\n",
    "\n",
    "        output_ids.append(next_token_id)\n",
    "\n",
    "        if next_token_id == vocab[\"<end>\"]:  # Stop if <end> is generated\n",
    "            break\n",
    "\n",
    "    # Convert token indices back to words\n",
    "    id_to_token = {idx: token for token, idx in vocab.items()}\n",
    "    translated_pseudocode = [id_to_token.get(idx, \"<unk>\") for idx in output_ids[1:]]  # Exclude <start>\n",
    "\n",
    "    return \" \".join(translated_pseudocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "t3HIGFZCKNaB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3HIGFZCKNaB",
    "outputId": "f9ac82d1-ff8a-45bc-aa06-9e78738a4021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 246086 examples for training\n"
     ]
    }
   ],
   "source": [
    "# Dataset Class\n",
    "class CPPToPseudoDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.inputs = [ast.literal_eval(x) for x in df['code_sequences']]\n",
    "        self.outputs = [ast.literal_eval(x) for x in df['text_sequences']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n",
    "        output_tensor = torch.tensor([vocab[\"<start>\"]] + self.outputs[idx] + [vocab[\"<end>\"]], dtype=torch.int64)\n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "# Padding Function\n",
    "def Add_Pad(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n",
    "    return inputs, outputs\n",
    "\n",
    "# Load Dataset\n",
    "dataset = CPPToPseudoDataset(\"C:/Users/shaiiikh/py/A3/tokenized_sequences.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=Add_Pad)\n",
    "\n",
    "print(f\"✅ Loaded {len(dataset)} examples for training\")\n",
    "\n",
    "# Training Configuration\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "\n",
    "# Create directory to save models\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "QXd9vz5EKT8u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXd9vz5EKT8u",
    "outputId": "692ad75d-6c39-4f68-e577-7bc88d4ccea3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3846/3846 [06:04<00:00, 10.56it/s, loss=1.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 1.4062\n",
      "✅ Model saved: checkpoints/cpp_to_pseudo_epoch_1.pth\n",
      "🔹 Example Prediction (C++ → Pseudocode): nan integer as nan integer nan integer nan integer nan integer nan integer nan integer as nan integer nan integer nan integer nan integer nan integer nan integer nan integer nan integer as nan integer as nan integer as nan integer nan integer nan integer nan integer nan integer nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(config.device), tgt.to(config.device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "\n",
    "        loss = criterion(output.view(-1, config.vocab_size), tgt_output.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save Model Checkpoint\n",
    "    torch.save(model.state_dict(), f\"checkpoints/cpp_to_pseudo_epoch_{epoch+1}.pth\")\n",
    "    print(f\"✅ Model saved: checkpoints/cpp_to_pseudo_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    # Print Example Prediction\n",
    "    model.eval()\n",
    "    example_cpp = [\"int\", \"main\", \"(\", \")\", \"{\", \"return\", \"0\", \";\", \"}\"]\n",
    "    translated_pseudocode = translate2(model, example_cpp, vocab, config.device)\n",
    "    print(f\"🔹 Example Prediction (C++ → Pseudocode): {translated_pseudocode}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "tfjIcOTkK33z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfjIcOTkK33z",
    "outputId": "2e481125-7130-4e79-c213-528162a252e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaiiikh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Example Prediction (C++ → Pseudocode): create integer a a with a = 10 a = 10 10 integer with a = 10 10 10 10 a 10 a 10 a 10 a 10 a 10 a 10 a 10 integer with a = 10 a 10 a 10 a 10 a 10 a 10 a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔹 Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "model = PseudoCodeTransformer(config).to(device)\n",
    "model.load_state_dict(torch.load(\"C:/Users/shaiiikh/py/A3/cpp_to_pseudo_epoch_1.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "example_cpp = [\"int\", \"a\",\"=\", \"10\",\";\"]\n",
    "translated_pseudocode = translate2(model, example_cpp, vocab, config.device)\n",
    "print(f\"🔹 Example Prediction (C++ → Pseudocode): {translated_pseudocode}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
